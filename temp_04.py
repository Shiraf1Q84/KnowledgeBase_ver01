import os
import streamlit as st
import openai
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, get_response_synthesizer
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main {
        font-family: Arial, sans-serif;
    }
    .stApp {
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
    }
    .search-container {
        display: flex;
        margin-bottom: 20px;
    }
    .search-input {
        flex-grow: 1;
        padding: 10px;
        font-size: 16px;
        border: 1px solid #dfe1e5;
        border-radius: 24px;
        outline: none;
    }
    .search-button {
        margin-left: 10px;
        padding: 10px 20px;
        font-size: 16px;
        background-color: #f8f9fa;
        border: 1px solid #f8f9fa;
        border-radius: 4px;
        color: #3c4043;
        cursor: pointer;
    }
    .search-button:hover {
        box-shadow: 0 1px 1px rgba(0,0,0,.1);
        background-color: #f8f9fa;
        border: 1px solid #dadce0;
        color: #202124;
    }
    .result-item {
        margin-bottom: 20px;
    }
    .result-title {
        color: #1a0dab;
        font-size: 18px;
        margin-bottom: 5px;
    }
    .result-url {
        color: #006621;
        font-size: 14px;
        margin-bottom: 5px;
    }
    .result-snippet {
        color: #545454;
        font-size: 14px;
    }
    .highlight {
        background-color: #ffffcc;
    }
</style>
""", unsafe_allow_html=True)

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="é«˜åº¦ãªé¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="collapsed",
)

# OpenAI APIã‚­ãƒ¼ã®è¨­å®š
if "api_key" not in st.session_state:
    st.session_state.api_key = ""

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«APIã‚­ãƒ¼å…¥åŠ›ã‚’é…ç½®
with st.sidebar:
    st.title("OpenAI API ã‚­ãƒ¼å…¥åŠ›")
    api_key = st.text_input("APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", value=st.session_state.api_key, type="password")
    if st.button("APIã‚­ãƒ¼ã‚’è¨­å®š"):
        st.session_state.api_key = api_key
        openai.api_key = api_key
        st.success("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ")

# ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰
@st.cache_resource(show_spinner=False)
def build_vector_database():
    with st.spinner(text="ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­..."):
        reader = SimpleDirectoryReader(input_dir="./data", recursive=True)
        docs = reader.load_data()
        embed_model = OpenAIEmbedding(openai_api_key=openai.api_key)
        index = VectorStoreIndex.from_documents(docs)
        return index

# ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹é–¢æ•°
def split_into_chunks(text, chunk_size=200, overlap=50):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

# ãƒãƒ£ãƒ³ã‚¯ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°é–¢æ•°
def score_chunks(chunks, query):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(chunks + [query])
    query_vector = tfidf_matrix[-1]
    chunk_vectors = tfidf_matrix[:-1]
    similarities = cosine_similarity(chunk_vectors, query_vector)
    return similarities.flatten()

# æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠã™ã‚‹é–¢æ•°
def select_best_chunks(chunks, scores, num_chunks=2):
    sorted_chunks = [chunk for _, chunk in sorted(zip(scores, chunks), reverse=True)]
    return sorted_chunks[:num_chunks]

# ãƒã‚¤ãƒ©ã‚¤ãƒˆé–¢æ•°
def highlight_text(text, query):
    words = query.lower().split()
    for word in words:
        text = re.sub(f'(?i){re.escape(word)}', lambda m: f'<span class="highlight">{m.group()}</span>', text)
    return text

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
if st.session_state.api_key:
    index = build_vector_database()
    
    st.title("é«˜åº¦ãªé¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢")
    
    # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form(key='search_form'):
        query = st.text_input("æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", key="search_input")
        submit_button = st.form_submit_button(label='æ¤œç´¢')
    
    if submit_button and query:
        with st.spinner("æ¤œç´¢ä¸­..."):
            # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®š
            retriever = VectorIndexRetriever(
                index=index,
                similarity_top_k=10,
            )
            response_synthesizer = get_response_synthesizer()
            query_engine = RetrieverQueryEngine(
                retriever=retriever,
                response_synthesizer=response_synthesizer,
                node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],
            )
            
            # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            response = query_engine.query(query)
            
            # æ¤œç´¢çµæœã®è¡¨ç¤º
            for i, node in enumerate(response.source_nodes, 1):
                content = node.node.get_content()
                score = node.score if hasattr(node, 'score') else 'N/A'
                file_name = node.node.metadata.get('file_name', 'ä¸æ˜')
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
                chunks = split_into_chunks(content)
                
                # ãƒãƒ£ãƒ³ã‚¯ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
                chunk_scores = score_chunks(chunks, query)
                
                # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠ
                best_chunks = select_best_chunks(chunks, chunk_scores)
                
                # é¸æŠã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                highlighted_content = "...".join([highlight_text(chunk, query) for chunk in best_chunks])
                
                # çµæœè¡¨ç¤º
                st.markdown(f"""
                <div class="result-item">
                    <div class="result-title">{file_name}</div>
                    <div class="result-url">é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {score:.2f}</div>
                    <div class="result-snippet">{highlighted_content}</div>
                </div>
                """, unsafe_allow_html=True)
                
                # å…ƒã®è³‡æ–™ã¸ã®ãƒªãƒ³ã‚¯
                file_path = os.path.join("data", file_name)
                if os.path.exists(file_path):
                    with open(file_path, "r") as f:
                        file_content = f.read()
                    st.download_button(
                        label=f"å…ƒã®è³‡æ–™ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=file_content,
                        file_name=file_name,
                        mime="text/plain",
                        key=f"download_button_{i}"
                    )
            
            # ç·åˆå›ç­”ã®è¡¨ç¤º
            st.subheader("ç·åˆå›ç­”:")
            st.write(response)
else:
    st.warning("OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’é–‹ã„ã¦è¨­å®šã—ã¦ãã ã•ã„ã€‚")